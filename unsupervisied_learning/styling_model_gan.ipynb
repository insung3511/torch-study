{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <F0D48035-EF9E-3141-9F63-566920E60D7C> /Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <44B645FB-F027-3EE5-86D7-DBF8E2FC6264> /Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.loss = F.mse_loss(x, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    a, b, c, d = x.size()\n",
    "    features = x.view(a * b, c * d)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_features):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_features).detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        G = gram_matrix(x)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return x\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = mean.view(-1, 1, 1)\n",
    "        self.std  = std.view(-1, 1, 1)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStyleModelAndLosses(cnn, styleImg, contentImg):\n",
    "    content_layers = ['conv_4']\n",
    "    style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "    normalization_mean = torch.tensor([0.485, 0.224, 0.225]).to(device)\n",
    "    normalization_std  = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "    normalization      = Normalization(normalization_mean, normalization_std).to(device)\n",
    "    contentLoss, styleLoss = [], []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    layerIndex = 0\n",
    "\n",
    "    for layer in cnn.children():\n",
    "        # Check the layer is available \n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            layerIndex += 1\n",
    "            name = 'conv_{}'.format(layerIndex)\n",
    "\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(layerIndex)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'maxpool_{}'.format(layerIndex)\n",
    "        \n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(layerIndex)\n",
    "        \n",
    "        # Excpetion case\n",
    "        else:\n",
    "            raise RuntimeError('Unreconginzed layer : {}'.format(layer.__class__.__name__))\n",
    "    \n",
    "        model.add_module(name, layer)\n",
    "        if name in content_layers:\n",
    "            target = model(contentImg)\n",
    "            contentLossItem = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(layerIndex), contentLossItem)\n",
    "            contentLoss.append(contentLossItem)\n",
    "\n",
    "        if name in style_layers:\n",
    "            targetFeature = model(styleImg)\n",
    "            styleLossItem = StyleLoss(targetFeature)\n",
    "            model.add_module(\"style_loss_{}\".format(layerIndex), styleLossItem)\n",
    "            styleLoss.append(styleLossItem)\n",
    "\n",
    "        for i in range(len(model) -1, -1, -1):\n",
    "            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "                break\n",
    "        \n",
    "        model = model[:(i + 1)]\n",
    "        return model, styleLoss, contentLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runStyleTransfer(cnn, contentImg, styleImg, num_steps=300, style_weight=100000, content_weight=1):\n",
    "    inputImg = contentImg.clone().detach().requires_grad_(True)\n",
    "    model, styleLosses, contentLosses = getStyleModelAndLosses(cnn, styleImg, contentImg)\n",
    "    optimizer = optim.LBFGS([inputImg])\n",
    "    iteration = [0]\n",
    "    \n",
    "    while iteration[0] <= num_steps:\n",
    "        def closuer():\n",
    "            inputImg.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model(inputImg)\n",
    "            \n",
    "            styleScore = 0\n",
    "            contentScore = 0\n",
    "            for sl in styleLosses:\n",
    "                styleScore += sl.loss\n",
    "            \n",
    "            for cl in contentLosses:\n",
    "                contentScore += cl.loss\n",
    "            \n",
    "            loss = (style_weight * styleScore) + (content_weight * contentScore)\n",
    "            loss.backward()\n",
    "            iteration[0] += 1\n",
    "            if iteration[0] % 50 == 0:\n",
    "                print('Iteration {}: Style Loss {:4f}\\tContent Loss : {:4f}'.format((\n",
    "                    iteration[0], styleScore.item(), contentScore.item()\n",
    "                )))\n",
    "            # print(styleScore + contentScore) \n",
    "            return (styleScore + contentScore)\n",
    "        optimizer.step(closuer)\n",
    "    return inputImg.data.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageLoader(image_path):\n",
    "    loader = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def imshow(image, title):\n",
    "    unloader = transforms.ToPILImage()\n",
    "    image = unloader(image.squeeze(0).cpu())\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "styleImg = imageLoader(\"../data/imgA.jpeg\")\n",
    "contentImg = imageLoader(\"../data/imgB.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m runStyleTransfer(cnn, contentImg\u001b[39m=\u001b[39;49mcontentImg, styleImg\u001b[39m=\u001b[39;49mstyleImg)\n",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m, in \u001b[0;36mrunStyleTransfer\u001b[0;34m(cnn, contentImg, styleImg, num_steps, style_weight, content_weight)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[39m# print(styleScore + contentScore) \u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[39mreturn\u001b[39;00m (styleScore \u001b[39m+\u001b[39m contentScore)\n\u001b[0;32m---> 30\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closuer)\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m inputImg\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[1;32m    434\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 438\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[1;32m    439\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    440\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[56], line 26\u001b[0m, in \u001b[0;36mrunStyleTransfer.<locals>.closuer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m iteration[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m iteration[\u001b[39m0\u001b[39m] \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: Style Loss \u001b[39m\u001b[39m{:4f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mContent Loss : \u001b[39m\u001b[39m{:4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat((\n\u001b[0;32m---> 26\u001b[0m         iteration[\u001b[39m0\u001b[39m], styleScore\u001b[39m.\u001b[39mitem(), contentScore\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     27\u001b[0m     )))\n\u001b[1;32m     28\u001b[0m \u001b[39m# print(styleScore + contentScore) \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m (styleScore \u001b[39m+\u001b[39m contentScore)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "output = runStyleTransfer(cnn, contentImg=contentImg, styleImg=styleImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m imshow(output, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOutput Image\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "imshow(output, title=\"Output Image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbd03b52000256fffc5622fb1d5afa03ae770321afbfaac74e08013d54c137c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
