{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <F0D48035-EF9E-3141-9F63-566920E60D7C> /Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <44B645FB-F027-3EE5-86D7-DBF8E2FC6264> /Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "EPOCH = 100\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128), transforms.ToTensor()])\n",
    "\n",
    "trainset = datasets.STL10(root='../data/', split='train', download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testset = datasets.STL10(root='../data/', split='test', download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/bahk_insung/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model = model.to(device)\n",
    "# model.load_state_dict(\n",
    "#     torch.load('../models/stl10_resnet18.pth')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0th Training: 100%|██████████| 125/125 [41:24<00:00, 19.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss : 1.7297169771194458\tTest Loss : 368.7901756989956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1th Training:  71%|███████   | 89/125 [1:45:21<29:31, 49.22s/it]   "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH + 1):\n",
    "    running_loss, test_loss = 0.0, 0.0\n",
    "    for data in tqdm(trainloader, desc=f'{epoch}th Training'):\n",
    "        image, label = data[0].to(device), data[1].to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                test_image, test_label = data[0].to(device), data[1].to(device)\n",
    "                output = model(test_image)\n",
    "                loss = criterion(output, test_label)\n",
    "                test_loss += loss.item()\n",
    "        model.train()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\\tLoss : {running_loss / len(trainloader)}\\tTest Loss : {test_loss / len(testloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9%\n",
      "Accuracy: 9%\n"
     ]
    }
   ],
   "source": [
    "def accuracyExprot(dataloader):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.detach(), 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(\"Accuracy: %d%%\" % (100 * correct / total))\n",
    "\n",
    "accuracyExprot(trainloader)\n",
    "accuracyExprot(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam(dataset, img_sample, img_size):\n",
    "    tmp = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.layer4[1].bn2.register_forward_hook(get_activation('final'))\n",
    "        data, label = dataset[img_sample]\n",
    "        data.unsqueeze_(0)\n",
    "        output = model(data.to(device))\n",
    "        _, prediction = torch.max(output, 1)\n",
    "        act = activation['final'].squeeze()\n",
    "        w = model.fc.weight\n",
    "        for idx in range(act.size(0)):\n",
    "            if idx == 0:\n",
    "                tmp = act[idx] * w[prediction.item()][idx]\n",
    "\n",
    "            else:\n",
    "                tmp += act[idx] * w[prediction.item()][idx]\n",
    "\n",
    "            normalization_cam = tmp.cpu().numpy()\n",
    "            normalization_cam = (normalization_cam - np.min(normalization_cam)) / (np.max(normalization_cam) - np.min(normalization_cam))\n",
    "            original_img = np.uint8((data[0][0] / 2 + 0.5) * 255)\n",
    "            cam_img = cv2.resize(np.uint8(normalization_cam * 255), dsize=(img_size, img_size))\n",
    "    return cam_img, original_img, prediction, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
